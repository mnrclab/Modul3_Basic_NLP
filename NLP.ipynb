{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic of **Natural Language Processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regresi --> prediksi angka -> gaji, usia, luas, \n",
    "Klasifikasi --> prediksi kelas -> sakit/sehat, laki-laki/perempuan, macet/lancar, ....\n",
    "Klustering ---> mengelompokkan dataset --> segmentasi pasar/pelanggan, dokumen, gambar, deteksi anomali\n",
    "\n",
    "----------------\n",
    "\n",
    "NLP --> menggunakan algoritma ML untuk menganalisis teks atau suara\n",
    "\n",
    "----------------\n",
    "\n",
    "Penerapan NLP:\n",
    "1. Perbankan - CV ---> \n",
    "2. Deteksi kepribadian melalui social media\n",
    "3. Translating/alih bahasa\n",
    "4. Text Prediction\n",
    "5. Analisis basis massa politik\n",
    "6. Deteksi berita hoax \n",
    "7. Sentiment analisys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The essence of Natural Language Processing lies in making computers understand the natural language. That’s not an easy task though. Computers can understand the structured form of data like spreadsheets and the tables in the database, but human languages, texts, and voices form an unstructured category of data, and it gets difficult for the computer to understand it, and there arises the need for Natural Language Processing.\n",
    "\n",
    "``NLP is used to apply machine learning algorithms to text and speech.``\n",
    "\n",
    "There’s a lot of natural language data out there in various forms and it would get very easy if computers can understand and process that data. We can train the models in accordance with expected output in different ways. Humans have been writing for thousands of years, there are a lot of literature pieces available, and it would be great if we make computers understand that. But the task is never going to be easy. There are various challenges floating out there like understanding the correct meaning of the sentence, correct Named-Entity Recognition(NER), correct prediction of various parts of speech, coreference resolution(the most challenging thing in my opinion).\n",
    "\n",
    "Computers can’t truly understand the human language. If we feed enough data and train a model properly, it can distinguish and try categorizing various parts of speech(noun, verb, adjective, supporter, etc…) based on previously fed data and experiences. If it encounters a new word it tried making the nearest guess which can be embarrassingly wrong few times.\n",
    "\n",
    "## **Use Cases of NLP**\n",
    "\n",
    "In simple terms, NLP represents the automatic handling of natural human language like speech or text, and although the concept itself is fascinating, the real value behind this technology comes from the use cases.\n",
    "\n",
    "NLP can help you with lots of tasks and the fields of application just seem to increase on a daily basis. Let’s mention some examples:\n",
    "\n",
    "-    NLP enables the recognition and prediction of diseases based on electronic health records and patient’s own speech. This capability is being explored in health conditions that go from cardiovascular diseases to depression and even schizophrenia. For example, Amazon Comprehend Medical is a service that uses NLP to extract disease conditions, medications, and treatment outcomes from patient notes, clinical trial reports, and other electronic health records.\n",
    "-    Organizations can determine what customers are saying about a service or product by identifying and extracting information in sources like social media. This sentiment analysis can provide a lot of information about customers choices and their decision drivers.\n",
    "-    An inventor at IBM developed a cognitive assistant that works like a personalized search engine by learning all about you and then remind you of a name, a song, or anything you can’t remember the moment you need it to.\n",
    "-    Companies like Yahoo and Google filter and classify your emails with NLP by analyzing text in emails that flow through their servers and stopping spam before they even enter your inbox.\n",
    "-    To help to identify fake news, the NLP Group at MIT developed a new system to determine if a source is accurate or politically biased, detecting if a news source can be trusted or not.\n",
    "-    Amazon’s Alexa and Apple’s Siri are examples of intelligent voice driven interfaces that use NLP to respond to vocal prompts and do everything like find a particular shop, tell us the weather forecast, suggest the best route to the office, or turn on the lights at home.\n",
    "-    Having insight into what is happening and what people are talking about can be very valuable to financial traders. NLP is being used to track news, reports, comments about possible mergers between companies, everything can be then incorporated into a trading algorithm to generate massive profits. Remember: buy the rumor, sell the news.\n",
    "-    NLP is also being used in both the search and selection phases of talent recruitment, identifying the skills of potential hires and also spotting prospects before they become active on the job market.\n",
    "-    Powered by IBM Watson NLP technology, LegalMation developed a platform to automate routine litigation tasks and help legal teams save time, drive down costs and shift strategic focus.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Overview**\n",
    "\n",
    "1.    NLP is an area of machine learning focused on teaching computers to understand natural human language better.\n",
    "2.    NLP draws on research from AI, but also from linguistics, mathematics, psychology, and other fields.\n",
    "3.    NLP enables computer programs to understand unstructured data, to make inferences and provide context to language, just a human brain does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introduction to the NLTK library for Python**\n",
    "\n",
    "``NLTK (Natural Language Toolkit) is a leading platform for building Python programs to work with human language data.``\n",
    "\n",
    "It provides easy-to-use interfaces to many corpora and lexical resources. Also, it contains a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning. Best of all, NLTK is a free, open source, community-driven project.\n",
    "\n",
    "We’ll use this toolkit to show some basics of the natural language processing field. For the examples below, I’ll assume that we have imported the NLTK toolkit. We can do this like this: ``import nltk.``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **The Basics of NLP for Text**\n",
    "\n",
    "In this article, we’ll cover the following topics:\n",
    "\n",
    "1.    Sentence Tokenization\n",
    "2.    Word Tokenization\n",
    "3.    Text Lemmatization and Stemming\n",
    "4.    Stop Words\n",
    "5.    Regex\n",
    "6.    Bag-of-Words\n",
    "7.    TF-IDF\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MINI GROUP ACTIVITY: The Basics of NLP for Text**\n",
    "\n",
    "### Every group have to discus these following topics (**WHAT, WHY, & HOW** (example of code)):\n",
    "\n",
    "#### 1.    **Group 1**: Sentence Tokenization & Word Tokenization, Regex (Regular Expression)\n",
    "#### 2.    **Group 2**: Text Lemmatization and Stemming\n",
    "#### 3.    **Group 3**: Stop Words, Bag-of-Words & TF-IDF\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Sentence Tokenization**\n",
    "\n",
    "``Sentence tokenization (also called sentence segmentation) is the problem of dividing a string of written language into its component sentences.``\n",
    "\n",
    "The idea here looks very simple. In English and some other languages, we can split apart the sentences whenever we see a punctuation mark.\n",
    "\n",
    "However, even in English, this problem is not trivial due to the use of full stop character for abbreviations. When processing plain text, tables of abbreviations that contain periods can help us to prevent incorrect assignment of sentence boundaries. In many cases, we use libraries to do that job for us, so don’t worry too much for the details for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backgammon is one of the oldest known board games.\n",
      "\n",
      "Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East.\n",
      "\n",
      "It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"Backgammon is one of the oldest known board games. Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East. It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice.\"\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "for i in sentences:\n",
    "    print(i + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Word Tokenization**\n",
    "\n",
    "``Word tokenization (also called word segmentation) is the problem of dividing a string of written language into its component words.``\n",
    "\n",
    "In English and many other languages using some form of Latin alphabet, space is a good approximation of a word divider.\n",
    "\n",
    "However, we still can have problems if we only split by space to achieve the wanted results. Some English compound nouns are variably written and sometimes they contain a space. In most cases, we use a library to achieve the wanted results, so again don’t worry too much for the details.\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "``Tokenization is the process of segmenting running text into sentences and words. In essence, it’s the task of cutting a text into pieces called tokens, and at the same time throwing away certain characters, such as punctuation. Following our example, the result of tokenization would be:``\n",
    "\n",
    "<img src = 'f_img.png'>\n",
    "\n",
    "Pretty simple, right? Well, although it may seem quite basic in this case and also in languages like English that separate words by a blank space (called segmented languages) not all languages behave the same, and if you think about it, blank spaces alone are not sufficient enough even for English to perform proper tokenizations. Splitting on blank spaces may break up what should be considered as one token, as in the case of certain names (e.g. San Francisco or New York) or borrowed foreign phrases (e.g. laissez faire).\n",
    "\n",
    "Tokenization can remove punctuation too, easing the path to a proper word segmentation but also triggering possible complications. In the case of periods that follow abbreviation (e.g. dr.), the period following that abbreviation should be considered as part of the same token and not be removed.\n",
    "\n",
    "The tokenization process can be particularly problematic when dealing with biomedical text domains which contain lots of hyphens, parentheses, and other punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Backgammon', 'is', 'one', 'of', 'the', 'oldest', 'known', 'board', 'games', '.']\n",
      "\n",
      "['Its', 'history', 'can', 'be', 'traced', 'back', 'nearly', '5,000', 'years', 'to', 'archeological', 'discoveries', 'in', 'the', 'Middle', 'East', '.']\n",
      "\n",
      "['It', 'is', 'a', 'two', 'player', 'game', 'where', 'each', 'player', 'has', 'fifteen', 'checkers', 'which', 'move', 'between', 'twenty-four', 'points', 'according', 'to', 'the', 'roll', 'of', 'two', 'dice', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"Backgammon is one of the oldest known board games. Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East. It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice.\"\n",
    "\n",
    "for i in sentences:\n",
    "    words = nltk.word_tokenize(i)\n",
    "    print(words)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Text Lemmatization and Stemming**\n",
    "\n",
    "For grammatical reasons, documents can contain different forms of a word such as drive, drives, driving. Also, sometimes we have related words with a similar meaning, such as nation, national, nationality.\n",
    "\n",
    "``The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.``\n",
    "\n",
    "Source: https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\n",
    "\n",
    "Examples:\n",
    "\n",
    "-    am, are, is => be\n",
    "-    dog, dogs, dog’s, dogs’ => dog\n",
    "\n",
    "The result of this mapping applied on a text will be something like that:\n",
    "\n",
    "-    the boy’s dogs are different sizes => the boy dog be differ size\n",
    "\n",
    "Stemming and lemmatization are special cases of normalization. However, they are different from each other.\n",
    "\n",
    "> *Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes.*\n",
    "\n",
    "> *Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.*\n",
    "\n",
    "Source: https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\n",
    "\n",
    "The difference is that a stemmer operates without knowledge of the context, and therefore cannot understand the difference between words which have different meaning depending on part of speech. But the stemmers also have some advantages, they are easier to implement and usually run faster. Also, the reduced “accuracy” may not matter for some applications.\n",
    "\n",
    "Examples:\n",
    "\n",
    "-    The word “better” has “good” as its lemma. This link is missed by stemming, as it requires a dictionary look-up.\n",
    "-    The word “play” is the base form for the word “playing”, and hence this is matched in both stemming and lemmatization.\n",
    "-    The word “meeting” can be either the base form of a noun or a form of a verb (“to meet”) depending on the context; e.g., “in our last meeting” or “We are meeting again tomorrow”. Unlike stemming, lemmatization attempts to select the correct lemma depending on the context.\n",
    "\n",
    "<hr>\n",
    "\n",
    "> ## **Stemming**\n",
    "\n",
    "Refers to the process of slicing the end or the beginning of words with the intention of removing affixes (lexical additions to the root of the word).\n",
    "\n",
    "<img src = 'g_img.png'>\n",
    "\n",
    "*Stemmers are simple to use and run very fast (they perform simple operations on a string), and if speed and performance are important in the NLP model, then stemming is certainly the way to go. Remember, we use it with the objective of improving our performance, not as a grammar exercise.*\n",
    "\n",
    "> ## **Lemmatization**\n",
    "\n",
    "Has the objective of reducing a word to its base form and grouping together different forms of the same word. For example, verbs in past tense are changed into present (e.g. “went” is changed to “go”) and synonyms are unified (e.g. “best” is changed to “good”), hence standardizing words with similar meaning to their root. Although it seems closely related to the stemming process, lemmatization uses a different approach to reach the root forms of words.\n",
    "\n",
    "*Lemmatization resolves words to their dictionary form (known as lemma) for which it requires detailed dictionaries in which the algorithm can look into and link words to their corresponding lemmas.*\n",
    "\n",
    "<img src = 'h_img.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer\n",
      "cat\n",
      "troubl\n",
      "troubl\n",
      "troubl\n",
      "Lancaster Stemmer\n",
      "cat\n",
      "troubl\n",
      "troubl\n",
      "troubl\n"
     ]
    }
   ],
   "source": [
    "print(\"Porter Stemmer\")\n",
    "print(porter.stem(\"cats\"))\n",
    "print(porter.stem(\"trouble\"))\n",
    "print(porter.stem(\"troubling\"))\n",
    "print(porter.stem(\"troubled\"))\n",
    "\n",
    "print(\"Lancaster Stemmer\")\n",
    "print(lancaster.stem(\"cats\"))\n",
    "print(lancaster.stem(\"trouble\"))\n",
    "print(lancaster.stem(\"troubling\"))\n",
    "print(lancaster.stem(\"troubled\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD                PORTER STEMMER      LANCASTER STEMMER   \n",
      "friend              friend              friend              \n",
      "friendship          friendship          friend              \n",
      "friends             friend              friend              \n",
      "friendships         friendship          friend              \n",
      "stabil              stabil              stabl               \n",
      "destabilize         destabil            dest                \n",
      "misunderstanding    misunderstand       misunderstand       \n",
      "railroad            railroad            railroad            \n",
      "moonlight           moonlight           moonlight           \n",
      "football            footbal             footbal             \n"
     ]
    }
   ],
   "source": [
    "#A list of words to be stemmed\n",
    "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"stabil\",\"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\"football\"]\n",
    "print(\"{0:20}{1:20}{2:20}\".format(\"WORD\",\"PORTER STEMMER\",\"LANCASTER STEMMER\"))\n",
    "\n",
    "for word in word_list:\n",
    "    print(\"{0:20}{1:20}{2:20}\".format(word, porter.stem(word), lancaster.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_stemmer_and_lemmatizer(stemmer, lemmatizer, word, pos):\n",
    "    \"\"\"\n",
    "    Print the results of stemmind and lemmitization using the passed stemmer, lemmatizer, word and pos (part of speech)\n",
    "    \"\"\"\n",
    "    print(\"Stemmer:\", stemmer.stem(word))\n",
    "    print(\"Lemmatizer:\", lemmatizer.lemmatize(word, pos))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmer: seen\n",
      "Lemmatizer: see\n",
      "\n",
      "Stemmer: drove\n",
      "Lemmatizer: drive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "compare_stemmer_and_lemmatizer(stemmer, lemmatizer, word = \"seen\", pos = wordnet.VERB)\n",
    "compare_stemmer_and_lemmatizer(stemmer, lemmatizer, word = \"drove\", pos = wordnet.VERB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Stop words**\n",
    "\n",
    "``Stop words are words which are filtered out before or after processing of text. They usually refer to the most common words in a language.`` \n",
    "\n",
    "When applying machine learning to text, these words can add a lot of noise. That’s why we want to remove these irrelevant words.\n",
    "\n",
    "Stop words usually refer to the most common words such as “and”, “the”, “a” in a language, but there is no single universal list of stopwords. The list of the stop words can change depending on your application.\n",
    "\n",
    "The NLTK tool has a predefined list of stopwords that refers to the most common words. If you use it for your first time, you need to download the stop words using this code: ``nltk.download(“stopwords”)``. Once we complete the downloading, we can load the ``stopwords package`` from the ``nltk.corpus`` and use it to load the stop words.\n",
    "\n",
    "``Stop Words Removal. Includes getting rid of common language articles, pronouns and prepositions such as “and”, “the” or “to” in English. In this process some very common words that appear to provide little or no value to the NLP objective are filtered and excluded from the text to be processed, hence removing widespread and frequent terms that are not informative about the corresponding text.``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Backgammon', 'one', 'oldest', 'known', 'board', 'games', '.']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "sentence = \"Backgammon is one of the oldest known board games.\"\n",
    "\n",
    "words = nltk.word_tokenize(sentence)\n",
    "without_stop_words = [i for i in words if not i in stop_words]\n",
    "print(without_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Regex**\n",
    "``A regular expression is a sequence of characters that define a search pattern.``\n",
    "\n",
    "A regular expression, regex, or regexp is a sequence of characters that define a search pattern. Let’s see some basics.\n",
    "\n",
    "-    . - match any character except newline\n",
    "-    \\w - match word\n",
    "-    \\d - match digit\n",
    "-    \\s - match whitespace\n",
    "-    \\W - match not word\n",
    "-    \\D - match not digit\n",
    "-    \\S - match not whitespace\n",
    "-    [abc] - match any of a, b, or c\n",
    "-    [^abc] - not match a, b, or c\n",
    "-    [a-g] - match a character between a & g\n",
    "\n",
    "*Regular expressions use the backslash character ('\\') to indicate special forms or to allow special characters to be used without invoking their special meaning. This collides with Python’s usage of the same character for the same purpose in string literals; for example, to match a literal backslash, one might have to write '\\\\\\\\' as the pattern string, because the regular expression must be \\\\, and each backslash must be expressed as \\\\ inside a regular Python string literal.*\n",
    "\n",
    "*The solution is to use Python’s raw string notation for regular expression patterns; backslashes are not handled in any special way in a string literal prefixed with 'r'. So r\"\\n\" is a two-character string containing '\\' and 'n', while \"\\n\" is a one-character string containing a newline. Usually, patterns will be expressed in Python code using this raw string notation.*\n",
    "\n",
    "Source: https://docs.python.org/3/library/re.html?highlight=regex\n",
    "\n",
    "We can use regex to apply additional filtering to our text. For example, we can remove all the non-words characters. In many cases, we don’t need the punctuation marks and it’s easy to remove them with regex.\n",
    "\n",
    "In Python, the ``re`` module provides regular expression matching operations similar to those in Perl. We can use the ``re.sub`` function to replace the matches for a pattern with a replacement string. Let’s see an example when we replace all non-words with the space character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The development of snowboarding was inspired by skateboarding  sledding  surfing and skiing \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "sentence = \"The development of snowboarding was inspired by skateboarding, sledding, surfing and skiing.\"\n",
    "pattern = r\"[^\\w]\"\n",
    "print(re.sub(pattern, \" \", sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Bag-of-words**\n",
    "\n",
    "``The bag-of-words model is a popular and simple feature extraction technique used when we work with text. It describes the occurrence of each word within a document``\n",
    "\n",
    "Machine learning algorithms cannot work with raw text directly, we need to convert the text into vectors of numbers. This is called feature extraction.\n",
    "\n",
    "The bag-of-words model is a popular and simple feature extraction technique used when we work with text. It describes the occurrence of each word within a document.\n",
    "\n",
    "To use this model, we need to:\n",
    "\n",
    "1.    Design a vocabulary of known words (also called tokens)\n",
    "2.    Choose a measure of the presence of known words\n",
    "\n",
    "Any information about the order or structure of words is discarded. That’s why it’s called a bag of words. This model is trying to understand whether a known word occurs in a document, but don’t know where is that word in the document.\n",
    "\n",
    "The intuition is that similar documents have similar contents. Also, from a content, we can learn something about the meaning of the document.\n",
    "\n",
    "<img src = 'e_img.png'>\n",
    "\n",
    "``Bag of Words Is a commonly used model that allows you to count all words in a piece of text. Basically it creates an occurrence matrix for the sentence or document, disregarding grammar and word order. These word frequencies or occurrences are then used as features for training a classifier.``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like this movie, it's funny.\n",
      "I hate this movie.\n",
      "This was awesome! I like it.\n",
      "Nice one. I love it.\n"
     ]
    }
   ],
   "source": [
    "with open(\"simple movie reviews.txt\", \"r\") as file:\n",
    "    documents = file.read().splitlines()\n",
    "    \n",
    "for i in documents:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s get all the unique words from the four loaded sentences ignoring the case, punctuation, and one-character tokens. These words will be our vocabulary (known words).\n",
    "\n",
    "We can use the **CountVectorizer** class from the sklearn library to design our vocabulary. We’ll see how we can use it after reading the next step, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries we need\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2. Design the Vocabulary\n",
    "# The default token pattern removes tokens of a single character. That's why we don't have the \"I\" and \"s\" tokens in the output\n",
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3. Create the Bag-of-Words Model\n",
    "bag_of_words = count_vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>awesome</th>\n",
       "      <th>funny</th>\n",
       "      <th>hate</th>\n",
       "      <th>it</th>\n",
       "      <th>like</th>\n",
       "      <th>love</th>\n",
       "      <th>movie</th>\n",
       "      <th>nice</th>\n",
       "      <th>one</th>\n",
       "      <th>this</th>\n",
       "      <th>was</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   awesome  funny  hate  it  like  love  movie  nice  one  this  was\n",
       "0        0      1     0   1     1     0      1     0    0     1    0\n",
       "1        0      0     1   0     0     0      1     0    0     1    0\n",
       "2        1      0     0   1     1     0      0     0    0     1    1\n",
       "3        0      0     0   1     0     1      0     1    1     0    0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the Bag-of-Words Model as a pandas DataFrame\n",
    "feature_names = count_vectorizer.get_feature_names()\n",
    "pd.DataFrame(bag_of_words.toarray(), columns = feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7. TF-IDF**\n",
    "\n",
    "``TF-IDF is a statistical measure used to evaluate the importance of a word to a document in a collection or corpus.``\n",
    "\n",
    "One problem with scoring word frequency is that the most frequent words in the document start to have the highest scores. These frequent words may not contain as much “informational gain” to the model compared with some rarer and domain-specific words. One approach to fix that problem is to penalize words that are frequent across all the documents. This approach is called TF-IDF.\n",
    "\n",
    "TF-IDF, short for **term frequency-inverse document frequency** is a statistical measure used to evaluate the importance of a word to a document in a collection or corpus.\n",
    "\n",
    "The TF-IDF scoring value increases proportionally to the number of times a word appears in the document, but it is offset by the number of documents in the corpus that contain the word.\n",
    "\n",
    "Let’s see the formula used to calculate a TF-IDF score for a given term x within a document y.\n",
    "\n",
    "<img src = a_img.png width=600 height=300>\n",
    "\n",
    "*TF-IDF Formula. Source: http://filotechnologia.blogspot.com/2014/01/a-simple-java-class-for-tfidf-scoring.html*\n",
    "\n",
    "Now, let’s split this formula a little bit and see how the different parts of the formula work.\n",
    "\n",
    "-    **Term Frequency (TF)**: a scoring of the frequency of the word in the current document.\n",
    "\n",
    "<img src = b_img.png>\n",
    "\n",
    "-    **Inverse Term Frequency (ITF)**: a scoring of how rare the word is across documents.\n",
    "\n",
    "<img src = c_img.png>\n",
    "\n",
    "-    Finally, we can use the previous formulas to calculate the **TF-IDF score** for a given term like this:\n",
    "\n",
    "<img src = d_img.png>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "values = tfidf_vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>awesome</th>\n",
       "      <th>funny</th>\n",
       "      <th>hate</th>\n",
       "      <th>it</th>\n",
       "      <th>like</th>\n",
       "      <th>love</th>\n",
       "      <th>movie</th>\n",
       "      <th>nice</th>\n",
       "      <th>one</th>\n",
       "      <th>this</th>\n",
       "      <th>was</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.571848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.365003</td>\n",
       "      <td>0.450852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.450852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.365003</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.702035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.553492</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.448100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.539445</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.344321</td>\n",
       "      <td>0.425305</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.344321</td>\n",
       "      <td>0.539445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.345783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.541736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.541736</td>\n",
       "      <td>0.541736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    awesome     funny      hate        it      like      love     movie  \\\n",
       "0  0.000000  0.571848  0.000000  0.365003  0.450852  0.000000  0.450852   \n",
       "1  0.000000  0.000000  0.702035  0.000000  0.000000  0.000000  0.553492   \n",
       "2  0.539445  0.000000  0.000000  0.344321  0.425305  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.345783  0.000000  0.541736  0.000000   \n",
       "\n",
       "       nice       one      this       was  \n",
       "0  0.000000  0.000000  0.365003  0.000000  \n",
       "1  0.000000  0.000000  0.448100  0.000000  \n",
       "2  0.000000  0.000000  0.344321  0.539445  \n",
       "3  0.541736  0.541736  0.000000  0.000000  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the Model as a pandas DataFrame\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "pd.DataFrame(values.toarray(), columns = feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Summary**\n",
    "\n",
    "In this blog post, you learn the basics of the NLP for text. More specifically you have learned the following concepts with additional details:\n",
    "\n",
    "-    NLP is used to apply machine learning algorithms to text and speech.\n",
    "-    NLTK (Natural Language Toolkit) is a leading platform for building Python programs to work with human language data\n",
    "-    Sentence tokenization is the problem of dividing a string of written language into its component sentences\n",
    "-    Word tokenization is the problem of dividing a string of written language into its component words\n",
    "-    The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.\n",
    "-    Stop words are words which are filtered out before or after processing of text. They usually refer to the most common words in a language.\n",
    "-    A regular expression is a sequence of characters that define a search pattern.\n",
    "-    The bag-of-words model is a popular and simple feature extraction technique used when we work with text. It describes the occurrence of each word within a document.\n",
    "-    TF-IDF is a statistical measure used to evaluate the importance of a word to a document in a collection or corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Cleaning Text Dataset for NLP\n",
    "\n",
    "## **Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Loading the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  liked\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_data = pd.read_csv(\"Restaurant_Reviews.tsv\", sep = \"\\t\")\n",
    "reviews_data.columns = [\"review\", \"liked\"]\n",
    "reviews_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_review(review):\n",
    "    \"\"\"\n",
    "    Receives a raw review and clean it using the following steps:\n",
    "    1. Remove all non-words\n",
    "    2. Transform the review in lower case\n",
    "    3. Remove all stop words\n",
    "    4. Perform stemming\n",
    "\n",
    "    Args:\n",
    "        review: the review that iwill be cleaned\n",
    "    Returns:\n",
    "        a clean review using the mentioned steps above.\n",
    "    \"\"\"\n",
    "    \n",
    "    review = re.sub(\"[^A-Za-z]\", \" \", review)\n",
    "    review = review.lower()\n",
    "    review = word_tokenize(review)\n",
    "    stemmer = PorterStemmer()\n",
    "    review = [stemmer.stem(word) for word in review if word not in set(stopwords.words(\"english\"))]\n",
    "    review = \" \".join(review)\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow... Loved this place.\n",
      "wow love place\n"
     ]
    }
   ],
   "source": [
    "review = reviews_data.review[0]\n",
    "print(review)\n",
    "\n",
    "cleaned_review = clean_review(review)\n",
    "print(cleaned_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wow love place',\n",
       " 'crust good',\n",
       " 'tasti textur nasti',\n",
       " 'stop late may bank holiday rick steve recommend love',\n",
       " 'select menu great price']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "for i in range(0, len(reviews_data)):\n",
    "    review = clean_review(reviews_data.review[i])\n",
    "    corpus.append(review)\n",
    "\n",
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Extracting a Features using the Bag-of-Words Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "features = count_vectorizer.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Reference**:\n",
    "- Adam Geitgey, \"Natural Language Processing is Fun!\", https://medium.com/@ageitgey/natural-language-processing-is-fun-9a0bff37854e\n",
    "- ODSC - Open Data Science, \"An Introduction to Natural Language Processing (NLP)\", https://medium.com/@ODSC/an-introduction-to-natural-language-processing-nlp-8e476d9f5f59\n",
    "- Ventsislav Yordanov, \"Introduction to Natural Language Processing for Text\", https://towardsdatascience.com/introduction-to-natural-language-processing-for-text-df845750fb63\n",
    "- Roger Chua, \"A simple way to explain Natural Language Processing (NLP)\", https://becominghuman.ai/a-simple-way-to-explain-natural-language-processing-nlp-b7147e68e663\n",
    "- Jaydeep1998, \"Introduction to Natural Language Processing\", https://www.geeksforgeeks.org/introduction-to-natural-language-processing/\n",
    "- Dhruv Kaushal 1, \"Processing text using NLP | Basics\", https://www.geeksforgeeks.org/processing-text-using-nlp-basics/\n",
    "- Diego Lopez Yse, \"Your Guide to Natural Language Processing (NLP)\", https://towardsdatascience.com/your-guide-to-natural-language-processing-nlp-48ea2511f6e1\n",
    "- NLP topic, https://towardsdatascience.com/tagged/nlp\n",
    "- Steming & Lemma, https://www.datacamp.com/community/tutorials/stemming-lemmatization-python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 32-bit",
   "language": "python",
   "name": "python38132bitf9f79e71b62e4503b25567c1d3914456"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
